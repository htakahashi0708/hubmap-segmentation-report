{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n\nThis is a notebook for a past kaggle competition [**HuBMAP - Hacking the Human Vasculature**.](https://www.kaggle.com/competitions/hubmap-hacking-the-human-vasculature) The goal of this competition is to detect Blood Vessels from images of kidney tissue taken by microscope, and detecton mask shall have [IoU (Intersection over Union)](https://learnopencv.com/intersection-over-union-iou-in-object-detection-and-segmentation/) greater than 0.6. The kaggle score is calculated by Average Precision Over Confidence which is the same as [Open Images 2019 - Instance Segmentation](https://www.kaggle.com/c/open-images-2019-instance-segmentation/overview/evaluation). \n\nThis HuBMAP competition seems to be difficult for few reasons. First, it looks almost impossible to correctly identify blood vessels by non-experts(refer to EDA). It would be hard for deep learning too. Second, only 1633 label data are given for images which is insufficient for such a complex segemtation. Another reason is imbalanced label: only 3.3% of image pixels are positive. Therefore, I defined following targets for this project. \n\n**Target**\n\n* To develop an effective training loop which includes **data augmentation** and **custom loss function**\n* To compare score and calculation time between two well-known model (**FCN and U-NET**)\n* **Average IoU > 0.5** in validation data when **20% of images are selected for validation**.\n* To submit prediction to kaggle","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\nimport time\nimport math\nimport json\n\nimport tensorflow as tf\nfrom keras import backend as K\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-09T05:50:27.268713Z","iopub.execute_input":"2023-09-09T05:50:27.269162Z","iopub.status.idle":"2023-09-09T05:50:35.670140Z","shell.execute_reply.started":"2023-09-09T05:50:27.269120Z","shell.execute_reply":"2023-09-09T05:50:35.669071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data\n\nThere are 1633 training images with their label (`polygon`), tile information (`tile_df`) which indicate datasource and source wsi. Source wsi is profiles of human subjects described in`wsi_df`.\n\n**Data Source**\n\nhttps://www.kaggle.com/competitions/hubmap-hacking-the-human-vasculature/data","metadata":{}},{"cell_type":"code","source":"train_folder = \"/kaggle/input/hubmap-hacking-the-human-vasculature/train/\"\ntest_folder = \"/kaggle/input/hubmap-hacking-the-human-vasculature/test/\"\nwsi_fpath = \"/kaggle/input/hubmap-hacking-the-human-vasculature/wsi_meta.csv\"\ntile_fpath = \"/kaggle/input/hubmap-hacking-the-human-vasculature/tile_meta.csv\"\nsample_fpath = \"/kaggle/input/hubmap-hacking-the-human-vasculature/sample_submission.csv\"\npolygon_fpath = \"/kaggle/input/hubmap-hacking-the-human-vasculature/polygons.jsonl\"","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:50:35.672120Z","iopub.execute_input":"2023-09-09T05:50:35.672914Z","iopub.status.idle":"2023-09-09T05:50:35.679060Z","shell.execute_reply.started":"2023-09-09T05:50:35.672879Z","shell.execute_reply":"2023-09-09T05:50:35.677954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(polygon_fpath) as f:\n    polygon = [json.loads(line) for line in f]\n    \nNP = len(polygon)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:50:35.680553Z","iopub.execute_input":"2023-09-09T05:50:35.681071Z","iopub.status.idle":"2023-09-09T05:50:40.099063Z","shell.execute_reply.started":"2023-09-09T05:50:35.681039Z","shell.execute_reply":"2023-09-09T05:50:40.098094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"N of train data = \", NP)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:50:40.102872Z","iopub.execute_input":"2023-09-09T05:50:40.103758Z","iopub.status.idle":"2023-09-09T05:50:40.109365Z","shell.execute_reply.started":"2023-09-09T05:50:40.103721Z","shell.execute_reply":"2023-09-09T05:50:40.108402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_files = os.listdir(train_folder)\ntile_df = pd.read_csv(tile_fpath)\nwsi_df = pd.read_csv(wsi_fpath)\nsample_df = pd.read_csv(sample_fpath)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:50:40.110821Z","iopub.execute_input":"2023-09-09T05:50:40.111473Z","iopub.status.idle":"2023-09-09T05:50:40.785743Z","shell.execute_reply.started":"2023-09-09T05:50:40.111433Z","shell.execute_reply":"2023-09-09T05:50:40.784843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tile_df","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:50:40.788479Z","iopub.execute_input":"2023-09-09T05:50:40.788820Z","iopub.status.idle":"2023-09-09T05:50:40.809831Z","shell.execute_reply.started":"2023-09-09T05:50:40.788792Z","shell.execute_reply":"2023-09-09T05:50:40.808985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wsi_df","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:50:40.811763Z","iopub.execute_input":"2023-09-09T05:50:40.812334Z","iopub.status.idle":"2023-09-09T05:50:40.825236Z","shell.execute_reply.started":"2023-09-09T05:50:40.812299Z","shell.execute_reply":"2023-09-09T05:50:40.824169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_fnames = []\nfor i in range(NP):\n    train_fnames.append(polygon[i][\"id\"])\n    \ntrain_name_df = pd.DataFrame({\"id\":train_fnames})\ntrain_name_df[\"count\"] = 1\n\ntile_df = pd.read_csv(tile_fpath)\ntile_df = tile_df.merge(train_name_df, on = \"id\", how = \"left\")\ntile_df = tile_df.query(\"count > 0\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:50:40.826964Z","iopub.execute_input":"2023-09-09T05:50:40.827456Z","iopub.status.idle":"2023-09-09T05:50:40.864480Z","shell.execute_reply.started":"2023-09-09T05:50:40.827425Z","shell.execute_reply":"2023-09-09T05:50:40.863639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading Images**\n\nThis code read images from folder. ","metadata":{}},{"cell_type":"code","source":"time1 = time.time()\ni = 0\nL = 512\nX_train = np.zeros((NP, L, L, 3), dtype =  np.uint8)\n\nfor i in range(NP):\n    #cv2 read images as BGR that should be converted into RGB\n    img = cv2.imread(train_folder + train_fnames[i] + \".tif\")[:,:,::-1]    \n    X_train[i,:,:,:] = img\n    \ntime2 = time.time()\ntime3 = np.round(time2 - time1)\nprint(time3, \"sec\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:50:40.865965Z","iopub.execute_input":"2023-09-09T05:50:40.866292Z","iopub.status.idle":"2023-09-09T05:51:26.170345Z","shell.execute_reply.started":"2023-09-09T05:50:40.866261Z","shell.execute_reply":"2023-09-09T05:51:26.169298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Cleaning\n\nQuality fo image is good and unnecessary to clean. The problem is that the label is given as polygon (geometry) data. For image segmentation, it shall be converted to pixelwise label. This operation is accomplished by [other notebook](https://www.kaggle.com/code/hidetaketakahashi/hubmap-create-mask). Put simply, it checks whether pixels are in polygon or not by breath-first-seach. BFS (starts from one of polygon element) significantly reduced calculation time compared with checking all the elements. ","metadata":{}},{"cell_type":"code","source":"# This data is pixel wise label creatated by other notebook\nlabel_mat = np.load(\"/kaggle/input/hubmap-label/mask_mat.npy\")\nlabel_mat[label_mat > 1] = 0\nlabel_mat = label_mat.reshape(NP, 512, 512, 1)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:26.175070Z","iopub.execute_input":"2023-09-09T05:51:26.175353Z","iopub.status.idle":"2023-09-09T05:51:28.525202Z","shell.execute_reply.started":"2023-09-09T05:51:26.175327Z","shell.execute_reply":"2023-09-09T05:51:28.524033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is code to copy polygon data into matrix \n\nnv = len(polygon[0][\"annotations\"])\n\nvmat =np.zeros((512, 512), dtype =  np.uint8)\n\nfor i in range(nv):\n    type1 = polygon[0][\"annotations\"][i][\"type\"]\n    if type1 == \"blood_vessel\":\n        crd = polygon[0][\"annotations\"][i][\"coordinates\"][0]\n        \n        for x, y in crd:\n            #pixels in polygon line is 1, otherwize 0\n            vmat[y, x] = 1","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:28.529821Z","iopub.execute_input":"2023-09-09T05:51:28.530206Z","iopub.status.idle":"2023-09-09T05:51:28.542733Z","shell.execute_reply.started":"2023-09-09T05:51:28.530165Z","shell.execute_reply":"2023-09-09T05:51:28.541590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following figure shows one of given image, its corresponding label, and pixel wise label.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,3, figsize = (10, 3))\n\nax[0].imshow(X_train[0])\nax[1].imshow(vmat)\nax[2].imshow(label_mat[0])\n\nax[0].set_title(\"Image\")\nax[1].set_title(\"given label polygon\")\nax[2].set_title(\"pixel wise label (preprocessed)\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:28.547733Z","iopub.execute_input":"2023-09-09T05:51:28.550214Z","iopub.status.idle":"2023-09-09T05:51:29.587019Z","shell.execute_reply.started":"2023-09-09T05:51:28.550180Z","shell.execute_reply":"2023-09-09T05:51:29.586114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. EDA","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Basic Information\n\nImages and labels are provided from two datasets and four sources (persons). Number of data is not equal between them. Kaggle test data is taken from only dataset1, but they are invisible for participants. ","metadata":{}},{"cell_type":"code","source":"select = [\"dataset\", \"source_wsi\", \"count\"]\n\nsum_data_df = tile_df[select].groupby(select[0:-1]).sum().astype(int)\nsum_data_df.plot(kind = \"bar\", title = \"count of (datast, source wsi)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:29.588113Z","iopub.execute_input":"2023-09-09T05:51:29.588436Z","iopub.status.idle":"2023-09-09T05:51:29.987037Z","shell.execute_reply.started":"2023-09-09T05:51:29.588406Z","shell.execute_reply":"2023-09-09T05:51:29.984935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_photo(img, title = \"\"):\n    \n    N = img.shape[0]\n    \n    NC = 5\n    NR =  math.ceil(N/NC)\n    fig, ax = plt.subplots(NR, NC, figsize = (12, NR*2.3))\n    \n    for k in range(N):\n        i =  int(k/NC)\n        j = k % NC\n        \n        if N <= 5:\n            ax[j].imshow(img[k])\n            ax[j].tick_params(left = False, right = False , labelleft = False ,\n                        labelbottom = False, bottom = False)\n        else:\n            ax[i,j].imshow(img[k])\n            ax[i,j].tick_params(left = False, right = False , labelleft = False ,\n                        labelbottom = False, bottom = False)\n    \n    if title != \"\":\n        if N <= 5:\n            ax[2].set_title(title)\n        else:\n            ax[0,2].set_title(title)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:29.989466Z","iopub.execute_input":"2023-09-09T05:51:29.990239Z","iopub.status.idle":"2023-09-09T05:51:30.000199Z","shell.execute_reply.started":"2023-09-09T05:51:29.990201Z","shell.execute_reply":"2023-09-09T05:51:29.998980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Images and Labels\n\nNext, images and labels are visualized by each dataset and source (person). Size and quantity of blood vessels are different between sources. ","metadata":{}},{"cell_type":"code","source":"#This code select some samples from each dataset and source\nnp.random.seed(1)\n\nsamples_ds = []\nfor ds in [1,2]:\n    filter1 = tile_df[\"dataset\"]==ds\n    samples = []\n    for wsi in [1,2,3,4]:\n        \n        if ds ==1:\n            if wsi in [1,2]:\n                filter2 = filter1 & (tile_df[\"source_wsi\"] == wsi)\n                s_idx = np.random.choice(np.where(filter2)[0], 50)\n                \n        else:\n            filter2 = filter1 & (tile_df[\"source_wsi\"] == wsi)\n            s_idx = np.random.choice(np.where(filter2)[0], 50)\n            \n        samples.append(s_idx)\n        \n    samples_ds.append(samples)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:30.001650Z","iopub.execute_input":"2023-09-09T05:51:30.002348Z","iopub.status.idle":"2023-09-09T05:51:30.013900Z","shell.execute_reply.started":"2023-09-09T05:51:30.002310Z","shell.execute_reply":"2023-09-09T05:51:30.012998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset1**\n\nIn the dataset 1, source 1 contains larger size but small number of blood vessels, whereas source 2 has many smaller size blood vessels. ","metadata":{}},{"cell_type":"code","source":"plot_photo(X_train[samples_ds[0][0][0:5]], \"dataset 1, source 1\")\nplot_photo(label_mat[samples_ds[0][0][0:5]], \"dataset 1, source 1, Label\")\nplot_photo(X_train[samples_ds[0][1][0:5]], \"dataset 1, source 2\")\nplot_photo(label_mat[samples_ds[0][1][0:5]], \"dataset 1, source 2, Label\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:30.015383Z","iopub.execute_input":"2023-09-09T05:51:30.015893Z","iopub.status.idle":"2023-09-09T05:51:33.853827Z","shell.execute_reply.started":"2023-09-09T05:51:30.015861Z","shell.execute_reply":"2023-09-09T05:51:33.852872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset 2**\n\nImage and labels of dataset 2 looks similar to dataset 1, but their color is slightly different. ","metadata":{}},{"cell_type":"code","source":"plot_photo(X_train[samples_ds[1][0][0:5]], \"dataset 2, source 1\")\nplot_photo(label_mat[samples_ds[1][0][0:5]], \"dataset 2, source 1, Label\")\n\nplot_photo(X_train[samples_ds[1][1][0:5]], \"dataset 2, source 2\")\nplot_photo(label_mat[samples_ds[1][1][0:5]], \"dataset 2, source 2, Label\")\n\nplot_photo(X_train[samples_ds[1][2][0:5]], \"dataset 2, source 3\")\nplot_photo(label_mat[samples_ds[1][2][0:5]], \"dataset 2, source 3, Label\")\n\nplot_photo(X_train[samples_ds[1][3][0:5]], \"dataset 2, source 4\")\nplot_photo(label_mat[samples_ds[1][3][0:5]], \"dataset 2, source 4, Label\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:33.855143Z","iopub.execute_input":"2023-09-09T05:51:33.856113Z","iopub.status.idle":"2023-09-09T05:51:41.833500Z","shell.execute_reply.started":"2023-09-09T05:51:33.856076Z","shell.execute_reply":"2023-09-09T05:51:41.832571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 Data Imbalance\n\nSince it was hard to clearly explain difference of distribution, following items are calculated by each image. Then distribution are plotted. \n\n* ratio of positive pixel (blood vessel)\n* Mean size and quantity of blood vessel\n\nThe function `label_stat` calculates number of blood vessel and mean size of them. OpenCV's function `cv2.connectedComponents` takes boolean image as input, and count number of connected area, then it assigns label into each of them in 2D matrix. `densty_plot` and `scatter_plot` visualize ratio of positive pixels, mean size and quantity of blood vessels respectively.","metadata":{}},{"cell_type":"code","source":"def label_stat(Y_mat):\n    \n    #n_labels = number of labels\n    # labels_mat= 2D matrix in which label of connected areas are assigned. \n    \n    n_labels, labels_mat = cv2.connectedComponents(Y_mat)\n    size_list = []\n    for i in range(1, n_labels):\n        #Calculate number of pixels for each label\n        filter1 = labels_mat == i\n        size = filter1.sum()\n        size_list.append(size)\n\n    return n_labels - 1, np.mean(size_list)\n\n\ndef density_plot(dataset, n_source):\n    \n    fig, ax = plt.subplots(1,n_source, figsize = (3.5*n_source,2.7))\n    \n    for i in range(n_source):\n        filter1 = (tile_df[\"dataset\"] == dataset) & (tile_df[\"source_wsi\"] == i + 1)\n        sns.kdeplot(tile_df[\"positive_ratio\"].loc[filter1], ax = ax[i])\n        ax[i].set_xlim(0, 0.5)\n        ax[i].set_title(\"(dataset, source) = (\" +str(dataset) + \", \" + str(i+1)  + \")\" )\n        if i > 0:\n            ax[i].set_ylabel(\"\")\n            \n    plt.show()\n    \n\ndef scatter_plot(dataset, n_source):\n    fig, ax = plt.subplots(1,n_source, figsize = (3.5*n_source,2.7))\n\n    for i in range(n_source):\n        filter1 = (tile_df[\"dataset\"] == dataset) & (tile_df[\"source_wsi\"] == i + 1)\n        sns.scatterplot(data = tile_df.loc[filter1], x =  \"n_BloodVessel\", y = \"meanSize_BloodVessel\", ax = ax[i], alpha = 0.4)\n        ax[i].set_xlim(0, 35)\n        ax[i].set_ylim(0, 15)\n        if i > 0:\n            ax[i].set_ylabel(\"\")\n        ax[i].set_title(\"(dataset, source) = (\" +str(dataset) + \", \" + str(i+1)  + \")\" )\n    \n    ax[0].set_ylabel(\"mean size (1000 pixel)\")\n    plt.show()\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:41.834982Z","iopub.execute_input":"2023-09-09T05:51:41.835535Z","iopub.status.idle":"2023-09-09T05:51:41.848289Z","shell.execute_reply.started":"2023-09-09T05:51:41.835499Z","shell.execute_reply":"2023-09-09T05:51:41.847359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_label_list = []\nmean_size_list = []\nratio_list = []\nfor i in range(NP):\n    n_label, mean_size = label_stat(label_mat[i])\n    n_label_list.append(n_label)\n    mean_size_list.append(mean_size)\n    ratio_list.append(np.mean(label_mat[i]))","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:41.849534Z","iopub.execute_input":"2023-09-09T05:51:41.850461Z","iopub.status.idle":"2023-09-09T05:51:46.645820Z","shell.execute_reply.started":"2023-09-09T05:51:41.850425Z","shell.execute_reply":"2023-09-09T05:51:46.644706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tile_df[\"n_BloodVessel\"] = n_label_list\ntile_df[\"meanSize_BloodVessel\"] = np.array(mean_size_list)/1000\ntile_df[\"positive_ratio\"] = ratio_list","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:46.647571Z","iopub.execute_input":"2023-09-09T05:51:46.647973Z","iopub.status.idle":"2023-09-09T05:51:46.663501Z","shell.execute_reply.started":"2023-09-09T05:51:46.647937Z","shell.execute_reply":"2023-09-09T05:51:46.657331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ratio of positive pixel\n\nOverall, only 3.3% of pixels are positive (blood vessel). When it is calculated by each dataset and source, dataset 1 has more positive pixels than dataset 2.","metadata":{}},{"cell_type":"code","source":"pct = np.round(tile_df[\"positive_ratio\"].mean()*100, 1)\nprint(pct, \"% of pixels are positive in all data\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:46.665369Z","iopub.execute_input":"2023-09-09T05:51:46.665783Z","iopub.status.idle":"2023-09-09T05:51:46.674325Z","shell.execute_reply.started":"2023-09-09T05:51:46.665729Z","shell.execute_reply":"2023-09-09T05:51:46.673030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tile_df[[\"dataset\", \"source_wsi\", \"positive_ratio\"]].groupby([\"dataset\", \"source_wsi\"]).mean()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:46.675867Z","iopub.execute_input":"2023-09-09T05:51:46.677022Z","iopub.status.idle":"2023-09-09T05:51:46.698789Z","shell.execute_reply.started":"2023-09-09T05:51:46.676985Z","shell.execute_reply":"2023-09-09T05:51:46.697603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ratio positive pixel is calculated by each image, and the distribution is plotted in following density plots.","metadata":{}},{"cell_type":"markdown","source":"\n\n**Dataset 1**","metadata":{}},{"cell_type":"code","source":"density_plot(1, 2)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:46.700527Z","iopub.execute_input":"2023-09-09T05:51:46.700861Z","iopub.status.idle":"2023-09-09T05:51:47.156343Z","shell.execute_reply.started":"2023-09-09T05:51:46.700830Z","shell.execute_reply":"2023-09-09T05:51:47.155432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset 2**","metadata":{}},{"cell_type":"code","source":"density_plot(2, 4)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:47.157543Z","iopub.execute_input":"2023-09-09T05:51:47.158819Z","iopub.status.idle":"2023-09-09T05:51:47.907520Z","shell.execute_reply.started":"2023-09-09T05:51:47.158783Z","shell.execute_reply":"2023-09-09T05:51:47.906597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Size and quantity of Blood Vessels\n\nDataset 1 source 1 has different distribution from others. It has less number of blood vessels, but their size are larger. Please note that **one dot corresponds to one image** in scatterplots.","metadata":{}},{"cell_type":"markdown","source":"**dataset1**","metadata":{}},{"cell_type":"code","source":"scatter_plot(1, 2)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:47.908890Z","iopub.execute_input":"2023-09-09T05:51:47.909327Z","iopub.status.idle":"2023-09-09T05:51:48.366594Z","shell.execute_reply.started":"2023-09-09T05:51:47.909293Z","shell.execute_reply":"2023-09-09T05:51:48.365635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**dataset2**","metadata":{}},{"cell_type":"code","source":"scatter_plot(2, 4)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:48.367960Z","iopub.execute_input":"2023-09-09T05:51:48.368281Z","iopub.status.idle":"2023-09-09T05:51:49.137922Z","shell.execute_reply.started":"2023-09-09T05:51:48.368255Z","shell.execute_reply":"2023-09-09T05:51:49.135112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Models\n\nFor image segmentation, Fully Convolutional Network and U-net are developed. Both of them are trained, then validation results are compared. ","metadata":{}},{"cell_type":"markdown","source":"## 5.1 FCN (Fully Convolutional Network)\n\nFully Convolutional Network (FCN) has two type of elements. First,`ConvBlock` which consists of `Conv2D` layer followed by `BatchNormalization` and `Relu`. It downsamples input by `MaxPooling2D` upon request. `ConvBlock` are allocated sequantially just like VGG16 or AlexNet architecture. \n\nUnlike architecture for image classificaiton, however, FCN does not have fully connected layer. Instead, it has `UpsampleBlock` which takes input of size 64x64x512, then generates 512x512x1 by `Conv2DTranspose`.","metadata":{}},{"cell_type":"code","source":"def ConvBlock(channel, X, ksize = 3, downsample = True):\n    \n    if downsample:\n        X =  layers.MaxPooling2D(pool_size=(2, 2), strides = (2,2))(X)        \n    \n    X = layers.Conv2D(channel, kernel_size = ksize, strides = 1, padding = \"same\")(X)        \n    X = layers.BatchNormalization()(X)\n    X = layers.ReLU()(X)\n    \n    return X\n\ndef UpsampleBlock(channel, X, ksize, stride):\n    \n    X = layers.Conv2DTranspose(channel, kernel_size=ksize, strides=stride, padding =\"same\")(X)\n        \n    return X","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:49.140877Z","iopub.execute_input":"2023-09-09T05:51:49.141163Z","iopub.status.idle":"2023-09-09T05:51:49.148315Z","shell.execute_reply.started":"2023-09-09T05:51:49.141137Z","shell.execute_reply":"2023-09-09T05:51:49.147301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_FCN():\n    \n    L = 512\n    Input =  layers.Input(shape=(L, L, 3))\n    \n    X = layers.Rescaling(scale = 1./127.5, offset= -1, )(Input)\n    \n    KS = 3\n    channel = 64\n    X = ConvBlock(channel, X, ksize = KS, downsample = False)#512\n    X = ConvBlock(channel, X, ksize = KS, downsample = False)#\n    X = ConvBlock(channel, X, ksize = KS, downsample = False)#\n    \n    channel = channel*2\n    X = ConvBlock(channel, X, ksize = KS, downsample = True)#256\n    X = ConvBlock(channel, X, ksize = KS, downsample = False)#\n    X = ConvBlock(channel, X, ksize = KS, downsample = False)#\n    channel = channel*2\n    X = ConvBlock(channel, X, ksize = KS, downsample = True)#128\n    X = ConvBlock(channel, X, ksize = KS, downsample = False)#\n    X = ConvBlock(channel, X, ksize = KS, downsample = False)#\n    channel = channel*2\n    X = ConvBlock(channel, X, ksize = KS, downsample = True)#64\n    X = ConvBlock(channel, X, ksize = KS, downsample = False)#\n    X = ConvBlock(channel, X, ksize = KS, downsample = False)#\n    \n    \n    X = UpsampleBlock(1, X, ksize = 8, stride = 8)\n\n    model = Model(inputs = Input, outputs = X)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:49.158906Z","iopub.execute_input":"2023-09-09T05:51:49.159486Z","iopub.status.idle":"2023-09-09T05:51:49.170039Z","shell.execute_reply.started":"2023-09-09T05:51:49.159450Z","shell.execute_reply":"2023-09-09T05:51:49.168955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_FCN = create_FCN()\nmodel_FCN.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:49.171348Z","iopub.execute_input":"2023-09-09T05:51:49.171874Z","iopub.status.idle":"2023-09-09T05:51:52.303514Z","shell.execute_reply.started":"2023-09-09T05:51:49.171838Z","shell.execute_reply":"2023-09-09T05:51:52.302705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model_FCN)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:52.304543Z","iopub.execute_input":"2023-09-09T05:51:52.305076Z","iopub.status.idle":"2023-09-09T05:51:52.655088Z","shell.execute_reply.started":"2023-09-09T05:51:52.305047Z","shell.execute_reply":"2023-09-09T05:51:52.654186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 U-NET\n\nU-net is basically similar to convolutional autoencoders that have downsampling decoder and upsampling encoder. What makes U-net unique is **skip connection from decoder to encoder at each level**. This structure looks similar to [V-model](https://en.wikipedia.org/wiki/V-model). \n\n`LeftBlock` written in the below code has `Conv2D` followed by `BatchNormalization` and `Relu`. It downsamples input by `MaxPooling2D` upon request. `RightBlock` upsamples input by `Conv2DTranspose` and it is concatenated with skip connection from `LeftBlock`. It is actually simple when plotted in the figure. ","metadata":{}},{"cell_type":"code","source":"def LeftBlock(channel, X, ksize = 3, downsample = True):\n    \n    if downsample:\n        X =  layers.MaxPooling2D(pool_size=(2, 2), strides = (2,2))(X)\n            \n    X = layers.Conv2D(channel, kernel_size = ksize, strides = 1, padding = \"same\")(X)\n    X = layers.BatchNormalization()(X)\n    X = layers.ReLU()(X)\n    \n    return X\n\n\ndef RightBlock(channel, X, ksize = 3, X_skip = None, upsample = True):\n    \n    if upsample:\n        X = layers.Conv2DTranspose(channel, kernel_size=4, strides=2, padding=\"SAME\")(X)\n    \n    if X_skip is not None:\n        X = layers.Concatenate()([X, X_skip])\n    \n    X =  layers.Conv2D(channel, kernel_size = ksize, strides = 1, padding = \"same\")(X)\n    X = layers.BatchNormalization()(X)\n    X = layers.ReLU()(X)\n\n    return X","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:52.656753Z","iopub.execute_input":"2023-09-09T05:51:52.657390Z","iopub.status.idle":"2023-09-09T05:51:52.667516Z","shell.execute_reply.started":"2023-09-09T05:51:52.657357Z","shell.execute_reply":"2023-09-09T05:51:52.666575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_UNET():\n    \n    L = 512\n    Input =  layers.Input(shape=(L, L, 3))\n\n    X0 = layers.Rescaling(scale = 1./127.5, offset= -1, )(Input)\n    \n    KS = 5\n    \n    channel = 48\n    X1 = LeftBlock(channel, X0, ksize = KS, downsample = False)#512\n\n    channel = channel*2 #128\n    X2 = LeftBlock(channel, X1, ksize = KS, downsample = True)#256\n\n    channel = channel*2 #256\n    X3 = LeftBlock(channel, X2, ksize = KS, downsample = True)#128\n    \n    channel = channel*2 #512\n    X4 = LeftBlock(channel, X3, ksize = KS, downsample = True)#64\n\n    channel = channel #512\n    X5 = LeftBlock(channel, X4, ksize = KS, downsample = True)#32\n    \n    XR = RightBlock(channel, X5, ksize = KS, X_skip = X4) #64\n\n    channel = int(channel/2) #256\n    XR = RightBlock(channel, XR, ksize = KS, X_skip = X3) #128\n\n    channel = int(channel/2) #128\n    XR = RightBlock(channel, XR, ksize = KS, X_skip = X2) #256\n\n    channel = int(channel/2) #64\n    XR = RightBlock(channel, XR, ksize = KS, X_skip = X1) #512\n\n    channel = 1\n    XR = layers.Conv2D(channel, kernel_size = 1, strides = 1, padding = \"same\")(XR)\n\n    model = Model(inputs = Input, outputs = XR)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:52.670296Z","iopub.execute_input":"2023-09-09T05:51:52.670621Z","iopub.status.idle":"2023-09-09T05:51:52.682738Z","shell.execute_reply.started":"2023-09-09T05:51:52.670597Z","shell.execute_reply":"2023-09-09T05:51:52.681407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_UNET = create_UNET()\nmodel_UNET.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:52.684166Z","iopub.execute_input":"2023-09-09T05:51:52.684537Z","iopub.status.idle":"2023-09-09T05:51:53.105454Z","shell.execute_reply.started":"2023-09-09T05:51:52.684494Z","shell.execute_reply":"2023-09-09T05:51:53.104716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Figure of U-net**","metadata":{}},{"cell_type":"code","source":"tf.keras.utils.plot_model(model_UNET)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:53.106606Z","iopub.execute_input":"2023-09-09T05:51:53.107056Z","iopub.status.idle":"2023-09-09T05:51:53.491154Z","shell.execute_reply.started":"2023-09-09T05:51:53.107028Z","shell.execute_reply":"2023-09-09T05:51:53.489989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Training and Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Split Dataset\n\n80% of image is randomly selected for training. ","metadata":{}},{"cell_type":"code","source":"batch_size = 4","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:53.492844Z","iopub.execute_input":"2023-09-09T05:51:53.493269Z","iopub.status.idle":"2023-09-09T05:51:53.498444Z","shell.execute_reply.started":"2023-09-09T05:51:53.493236Z","shell.execute_reply":"2023-09-09T05:51:53.497257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\n\nn_data = NP\nn_train = int(n_data*0.8)\n\nall_idx = np.arange(n_data)\nnp.random.shuffle(all_idx)\n\ntrain_idx = all_idx[:n_train]\nval_idx = all_idx[n_train:]\n\nprint(\"N of data for training = \", train_idx.shape[0]) \nprint(\"N of data for validation = \", val_idx.shape[0])","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:53.500248Z","iopub.execute_input":"2023-09-09T05:51:53.500645Z","iopub.status.idle":"2023-09-09T05:51:53.510308Z","shell.execute_reply.started":"2023-09-09T05:51:53.500611Z","shell.execute_reply":"2023-09-09T05:51:53.509136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((X_train[train_idx], label_mat[train_idx])).shuffle(1000).batch(batch_size)\n\nX_val = X_train[val_idx]\nY_val = label_mat[val_idx]\nn_val = X_val.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:53.512124Z","iopub.execute_input":"2023-09-09T05:51:53.512504Z","iopub.status.idle":"2023-09-09T05:51:56.501007Z","shell.execute_reply.started":"2023-09-09T05:51:53.512469Z","shell.execute_reply":"2023-09-09T05:51:56.499939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`val_tile_df` is later used to analyze validation result.","metadata":{}},{"cell_type":"code","source":"val_tile_df = tile_df.iloc[val_idx].reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.502594Z","iopub.execute_input":"2023-09-09T05:51:56.502955Z","iopub.status.idle":"2023-09-09T05:51:56.509146Z","shell.execute_reply.started":"2023-09-09T05:51:56.502921Z","shell.execute_reply":"2023-09-09T05:51:56.508230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 Custom Loss Function\n\nSince the label is imbalanced (refer to chapter 4 EDA), simple binary cross entropy would not work well. Instead, this `custom_loss` function calculate binary cross entropy for positive and negative pixels separately. Because tesorflow's `BCE` contains reduce_mean function, positive and negative losses can be balanced in this way. It is same as **weighted log loss**\nThen, positve loss and negative loss are summed with **weight of negative loss**. When the weight of negative loss is 2 or 3 , the result was good. ","metadata":{}},{"cell_type":"code","source":"# Custom Loss Function\n\nBCE = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef custom_loss(y_true, y_pred):\n\n    filter1 = y_true == 1\n\n    p_loss = BCE(y_true[filter1], y_pred[filter1])\n\n    filter1 = y_true == 0\n    n_loss = BCE(y_true[filter1], y_pred[filter1])\n\n    loss =  p_loss + n_loss*3 #weighted log loss\n\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.510838Z","iopub.execute_input":"2023-09-09T05:51:56.511510Z","iopub.status.idle":"2023-09-09T05:51:56.519909Z","shell.execute_reply.started":"2023-09-09T05:51:56.511475Z","shell.execute_reply":"2023-09-09T05:51:56.518786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3 Image Augmentation\n\nThe funciton `augment` is applied to both image and label, because when image is flipped, label shall follow it. First part of augmentation is **horizontal and vertical flip** at probability 0.5 respectively. Next is image rotation. **The rotation angle is selected from 0, 90, 180 and 270**. Finally, **the image and the label is zoomed** at 0.3 probability. It select uppler left pixel randomly. Then the length is selected. It is clipped and resized to 512x512. ","metadata":{}},{"cell_type":"code","source":"#Image Augment Function\ndef augment(X, Y):\n    \n    #1. random flip--------------\n    #1.1 horizontal\n    if tf.random.uniform(shape=[1]) > 0.5:\n        X = X[:,:,::-1]\n        Y = Y[:,:,::-1]\n        \n    #1.2 vertical\n    if tf.random.uniform(shape=[1]) > 0.5:\n        X = X[:,::-1]\n        Y = Y[:,::-1]\n    \n    #2. rotation------------------\n    #2.1 set angle \n    angle = np.random.randint(4) \n    X = tf.image.rot90(X, k=angle)\n    Y = tf.image.rot90(Y, k=angle)\n    \n    #3. zoom -----------------------------\n    L = 512\n    if tf.random.uniform(shape=[1]) < 0.3:\n        \n        #left upper\n        x1 = np.random.choice(np.arange(0, int(L*0.2)), 1)[0]\n        y1 = np.random.choice(np.arange(0, int(L*0.2)), 1)[0]\n        \n        #Length\n        L2 = min(L-x1, L-y1)\n        L3 = np.random.choice(np.arange(int(L2*0.8), L2), 1)[0]\n        \n        X = X[:,y1:(y1+L3), x1:(x1+L3),:] \n        Y = Y[:,y1:(y1+L3), x1:(x1+L3),:] \n        \n        X = tf.cast(tf.image.resize(X,[L, L]), dtype = tf.uint8)\n        Y = tf.cast(tf.image.resize(Y,[L, L]), dtype = tf.uint8)\n    \n    return X, Y","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.521622Z","iopub.execute_input":"2023-09-09T05:51:56.522022Z","iopub.status.idle":"2023-09-09T05:51:56.534486Z","shell.execute_reply.started":"2023-09-09T05:51:56.521990Z","shell.execute_reply":"2023-09-09T05:51:56.533291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 Custom train loop\n\nThe custom train loop `train_loop` consists of `augment` fuction, `train_step` which calculates loss and apply gradients, and some functions to calculate validation results (`cal_IoU`, `predict_probability`, `val_score`). The statement `@tf.function` before the `train_step` significantly accelerates training by creating computation graph. Furthermore, to keep the best model, `train_loop` saves trained model when validation loss is the lowest. ","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(X, Y, model):\n        \n    with tf.GradientTape() as tape:\n        Y_pred = model(X)\n        loss = custom_loss(Y, Y_pred)\n        \n        \n    grads = tape.gradient(loss, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.535859Z","iopub.execute_input":"2023-09-09T05:51:56.536485Z","iopub.status.idle":"2023-09-09T05:51:56.546860Z","shell.execute_reply.started":"2023-09-09T05:51:56.536452Z","shell.execute_reply":"2023-09-09T05:51:56.546100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculate IoU from predicted prob\ndef cal_IoU(Y_true, prob, cutoff = 0):\n    \n    Y_pred = (prob > cutoff).astype(int)\n    \n    score_list = []\n    for k in range(prob.shape[0]):\n    \n        and_score =  np.sum(Y_pred[k][Y_true[k] == 1])\n        or_score = np.sum(Y_true[k]) + np.sum(Y_pred[k]) - and_score\n        \n        \n        if or_score == 0:\n            score = 1\n        else:\n            score = and_score/or_score\n        score_list.append(score)\n    \n    return np.round(np.mean(np.array(score_list)), 3) \n\n\n#calculation of probability from logit\ndef predict_probability(X, model):\n    pred = model.predict(X, verbose = 0)\n    odd = np.exp(pred)\n    prob = odd/(1+odd)\n    \n    return prob\n    \n# validation loss \ndef val_score(X, Y, model):\n    \n    Y_pred = model.predict(X)\n    loss =  custom_loss(Y, Y_pred)\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.548039Z","iopub.execute_input":"2023-09-09T05:51:56.548986Z","iopub.status.idle":"2023-09-09T05:51:56.558242Z","shell.execute_reply.started":"2023-09-09T05:51:56.548950Z","shell.execute_reply":"2023-09-09T05:51:56.557624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(n_epoch, history, model, model_name, print_result = True):\n    \n    best_val_loss = 100000.\n    \n    for k in range(n_epoch):\n        \n        time1 = time.time()\n        \n        loss_list = []\n        for _, ds in enumerate(train_ds):\n            X, Y = ds #to extract X and Y from dataset\n\n            X, Y = augment(X, Y) #image augmentation\n            \n            loss = train_step(X, Y, model) #calculate loss and apply gradients\n            loss_list.append(loss)\n        \n        #recording loss\n        train_loss = np.mean(loss_list)    \n        val_loss = val_score(X_val, Y_val, model)\n        \n        #calculate IoU\n        prob = predict_probability(X_val, model)\n        val_IoU = cal_IoU(Y_val, prob, 0.8)\n        \n        #records loss and IoU in history\n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_IoU\"].append(val_IoU)\n        \n        time2 = time.time()\n        time3 = np.round(time2- time1)\n        \n        if k >= 0:\n            #keep saving the best model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                model.save_weights(model_name + \"/ckpt1\")\n                print(\"write model at epoch \", k)\n        \n        if print_result:\n            print(k, \"train loss\", train_loss, \", val loss \", val_loss, \", val_IoU\", val_IoU, \" time[s] = \", time3)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.559375Z","iopub.execute_input":"2023-09-09T05:51:56.560325Z","iopub.status.idle":"2023-09-09T05:51:56.571624Z","shell.execute_reply.started":"2023-09-09T05:51:56.560292Z","shell.execute_reply":"2023-09-09T05:51:56.570925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is the function to save train and validation history.\ndef save_history(history, model_name):\n    \n    np.save(\"train_loss_\" + model_name, np.array(history[\"train_loss\"]))\n    np.save(\"val_loss_\" + model_name, np.array(history[\"val_loss\"]))\n    np.save(\"val_IoU_\" + model_name, np.array(history[\"val_IoU\"]))","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.572767Z","iopub.execute_input":"2023-09-09T05:51:56.573700Z","iopub.status.idle":"2023-09-09T05:51:56.584261Z","shell.execute_reply.started":"2023-09-09T05:51:56.573645Z","shell.execute_reply":"2023-09-09T05:51:56.583498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.5 Hyperparameter Tuning\n\nAfter running notebooks many times, following hyperparameters were tuned. Only the results are shown here, because demonstrating all the process is not possible within limited calculation time. \n\n**General**\n\n* adam optimizer with learning rate = 0.0002. (much better than default lr 0.001) with batch size = 4\n* weight of negative loss (false positive) in `custom_loss` = 3\n* best epoch: 20 ~ 30 \n\n**FCN**\n* Number of conv block at each level = 3\n* Convolution kernel size = 3\n\n**U-NET**\n* Number of conv block at each level = 1\n* Convolution kernel size = 5","metadata":{}},{"cell_type":"markdown","source":"## 6.6 Training FCN","metadata":{}},{"cell_type":"markdown","source":"**This model is previously trained in the notebook to avoid OutOfMemoryError**, because training two models in one notebook was not possible. It loads trained results.","metadata":{}},{"cell_type":"code","source":"history_FCN = {}\nhistory_FCN[\"train_loss\"] = []\nhistory_FCN[\"val_loss\"] = []\nhistory_FCN[\"val_IoU\"] = []","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.585342Z","iopub.execute_input":"2023-09-09T05:51:56.586213Z","iopub.status.idle":"2023-09-09T05:51:56.594495Z","shell.execute_reply.started":"2023-09-09T05:51:56.586186Z","shell.execute_reply":"2023-09-09T05:51:56.593447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0002)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.596178Z","iopub.execute_input":"2023-09-09T05:51:56.596578Z","iopub.status.idle":"2023-09-09T05:51:56.608203Z","shell.execute_reply.started":"2023-09-09T05:51:56.596546Z","shell.execute_reply":"2023-09-09T05:51:56.607242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_loop(20, history_FCN, model_FCN, \"HuBMAP-FCN-model\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.609701Z","iopub.execute_input":"2023-09-09T05:51:56.610777Z","iopub.status.idle":"2023-09-09T05:51:56.614630Z","shell.execute_reply.started":"2023-09-09T05:51:56.610743Z","shell.execute_reply":"2023-09-09T05:51:56.613735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save_history(history_FCN, \"FCN\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.615931Z","iopub.execute_input":"2023-09-09T05:51:56.616892Z","iopub.status.idle":"2023-09-09T05:51:56.623726Z","shell.execute_reply.started":"2023-09-09T05:51:56.616857Z","shell.execute_reply":"2023-09-09T05:51:56.622813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**import trained model and result**","metadata":{}},{"cell_type":"code","source":"model_FCN.load_weights(\"/kaggle/input/hubmap-reportfiles/fcn_model/ckpt1\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:56.624987Z","iopub.execute_input":"2023-09-09T05:51:56.625796Z","iopub.status.idle":"2023-09-09T05:51:57.099018Z","shell.execute_reply.started":"2023-09-09T05:51:56.625764Z","shell.execute_reply":"2023-09-09T05:51:57.098090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_FCN[\"train_loss\"] = np.load(\"/kaggle/input/hubmap-reportfiles/fcn_loss/train_loss_FCN.npy\")\nhistory_FCN[\"val_loss\"] = np.load(\"/kaggle/input/hubmap-reportfiles/fcn_loss/val_loss_FCN.npy\")\nhistory_FCN[\"val_IoU\"] = np.load(\"/kaggle/input/hubmap-reportfiles/fcn_loss/val_IoU_FCN.npy\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:57.100538Z","iopub.execute_input":"2023-09-09T05:51:57.100910Z","iopub.status.idle":"2023-09-09T05:51:57.119192Z","shell.execute_reply.started":"2023-09-09T05:51:57.100876Z","shell.execute_reply":"2023-09-09T05:51:57.118331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(history):\n    \n    fig, ax = plt.subplots(figsize = (5,4))\n    ax.plot(history[\"train_loss\"], label = \"train loss\")    \n    ax.plot(history[\"val_loss\"], label = \"val loss\")\n    ax.set_title(\"loss\")\n    ax.legend()\n    ax.grid()\n    #fig, ax = plt.subplots(1,2, figsize = (10,4))\n    #ax[0].plot(history[\"train_loss\"], label = \"train loss\")    \n    #ax[0].plot(history[\"val_loss\"], label = \"val loss\")\n    \n    #ax[1].plot(history[\"val_IoU\"])\n    #ax[0].set_title(\"loss\")\n    #ax[0].legend()\n    #ax[1].set_title(\"val IoU (in all images)\")\n    #ax[0].grid()\n    #ax[1].grid()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:57.120690Z","iopub.execute_input":"2023-09-09T05:51:57.121052Z","iopub.status.idle":"2023-09-09T05:51:57.127293Z","shell.execute_reply.started":"2023-09-09T05:51:57.121019Z","shell.execute_reply":"2023-09-09T05:51:57.126197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history_FCN)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:57.129292Z","iopub.execute_input":"2023-09-09T05:51:57.129640Z","iopub.status.idle":"2023-09-09T05:51:57.474858Z","shell.execute_reply.started":"2023-09-09T05:51:57.129608Z","shell.execute_reply":"2023-09-09T05:51:57.473920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.7 Training U-NET","metadata":{}},{"cell_type":"markdown","source":"**This model is previously trained in the notebook to avoid OutOfMemoryError**, because training two models in one notebook was not possible. It loads trained results. ","metadata":{}},{"cell_type":"code","source":"history_UNET = {}\nhistory_UNET[\"train_loss\"] = []\nhistory_UNET[\"val_loss\"] = []\nhistory_UNET[\"val_IoU\"] = []","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:57.476362Z","iopub.execute_input":"2023-09-09T05:51:57.476765Z","iopub.status.idle":"2023-09-09T05:51:57.482396Z","shell.execute_reply.started":"2023-09-09T05:51:57.476729Z","shell.execute_reply":"2023-09-09T05:51:57.480667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_loop(20, history_UNET, model_UNET, \"HuBMAP-UNET-model\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:57.484037Z","iopub.execute_input":"2023-09-09T05:51:57.484456Z","iopub.status.idle":"2023-09-09T05:51:57.492448Z","shell.execute_reply.started":"2023-09-09T05:51:57.484421Z","shell.execute_reply":"2023-09-09T05:51:57.491314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save_history(history_UNET, \"UNET\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:57.494253Z","iopub.execute_input":"2023-09-09T05:51:57.494615Z","iopub.status.idle":"2023-09-09T05:51:57.501500Z","shell.execute_reply.started":"2023-09-09T05:51:57.494583Z","shell.execute_reply":"2023-09-09T05:51:57.500549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**import trained result**","metadata":{}},{"cell_type":"code","source":"model_UNET.load_weights(\"/kaggle/input/hubmap-reportfiles/unet_model/ckpt1\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:57.502848Z","iopub.execute_input":"2023-09-09T05:51:57.503110Z","iopub.status.idle":"2023-09-09T05:51:58.290350Z","shell.execute_reply.started":"2023-09-09T05:51:57.503087Z","shell.execute_reply":"2023-09-09T05:51:58.289465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_UNET[\"train_loss\"] = np.load(\"/kaggle/input/hubmap-reportfiles/unet_loss/train_loss_UNET.npy\")\nhistory_UNET[\"val_loss\"] = np.load(\"/kaggle/input/hubmap-reportfiles/unet_loss/val_loss_UNET.npy\")\nhistory_UNET[\"val_IoU\"] = np.load(\"/kaggle/input/hubmap-reportfiles/unet_loss/val_IoU_UNET.npy\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:58.291846Z","iopub.execute_input":"2023-09-09T05:51:58.292203Z","iopub.status.idle":"2023-09-09T05:51:58.310060Z","shell.execute_reply.started":"2023-09-09T05:51:58.292170Z","shell.execute_reply":"2023-09-09T05:51:58.309216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history_UNET)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:58.311335Z","iopub.execute_input":"2023-09-09T05:51:58.312041Z","iopub.status.idle":"2023-09-09T05:51:58.649853Z","shell.execute_reply.started":"2023-09-09T05:51:58.312008Z","shell.execute_reply":"2023-09-09T05:51:58.648991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Analysis of Validation Result\n\n## 7.1 Validation Loss\n\nThere are lots of ways to analyse results. First, validation loss of both models are compared. According to the plots, both model learned well but U-NET has slightly smaller loss than FCN.","metadata":{}},{"cell_type":"code","source":"def compare_val(history_FCN, history_UNET):\n    \n    fig, ax = plt.subplots(figsize = (6, 5))\n    ax.plot(history_FCN[\"val_loss\"], label = \"FCN val loss\")\n    ax.plot(history_UNET[\"val_loss\"], label = \"U-NET val loss\")\n    ax.set_title(\"val loss\")\n    ax.grid()\n    ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:58.651110Z","iopub.execute_input":"2023-09-09T05:51:58.651524Z","iopub.status.idle":"2023-09-09T05:51:58.657452Z","shell.execute_reply.started":"2023-09-09T05:51:58.651489Z","shell.execute_reply":"2023-09-09T05:51:58.656481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_val(history_FCN, history_UNET)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:58.658766Z","iopub.execute_input":"2023-09-09T05:51:58.659477Z","iopub.status.idle":"2023-09-09T05:51:58.955560Z","shell.execute_reply.started":"2023-09-09T05:51:58.659444Z","shell.execute_reply":"2023-09-09T05:51:58.954624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.2 Predicted labels\n\nSince both of FCN and U-net trained well, they must be able to predict label with good accuracy. The funciton `plot_result` visualize an image, its label, and prediction results by FCN and U-net.","metadata":{}},{"cell_type":"code","source":"def plot_result(X, Y_true, Y_pred1, Y_pred2, cutoff = 0.8):\n    \n    N = X.shape[0]\n    \n    fig, ax = plt.subplots(N,6, figsize = (13,2.2*N))\n    \n    for k in range(N):\n        \n        cutoff_img1 = (Y_pred1[k,:,:,0] > cutoff).astype(int)\n        cutoff_img2 = (Y_pred2[k,:,:,0] > cutoff).astype(int)\n\n        true_img = np.zeros((512, 512, 3), dtype = np.uint8)\n        true_img[:,:,1] = Y_true[k,:,:,0]*200\n        \n        cutoff1 = np.zeros((512, 512, 3), dtype = np.uint8)\n        cutoff2 = np.zeros((512, 512, 3), dtype = np.uint8)\n        \n        cutoff1[:,:,0] = cutoff_img1*230\n        cutoff2[:,:,0] = cutoff_img2*230\n        \n        cutoff1[:,:,1] = cutoff_img1*50\n        cutoff2[:,:,1] = cutoff_img2*50\n        cutoff1[:,:,2] = cutoff_img1*50\n        cutoff2[:,:,2] = cutoff_img2*50\n        \n        diff_photo1 = cutoff1.copy()\n        diff_photo2 = cutoff2.copy()\n        diff_photo1[:,:,1] += Y_true[k,:,:,0]*200\n        diff_photo2[:,:,1] += Y_true[k,:,:,0]*200\n        \n        ax[k, 0].imshow(X[k])\n        ax[k, 1].imshow(true_img, cmap = \"gray\")\n        ax[k, 2].imshow(cutoff1, cmap = \"gray\")\n        ax[k, 3].imshow(diff_photo1)\n        ax[k, 4].imshow(cutoff2, cmap = \"gray\")\n        ax[k, 5].imshow(diff_photo2)\n        \n        for j in range(6):\n            ax[k,j].set_xticks([])\n            ax[k,j].set_yticks([])\n    \n        if k == 0:\n            ax[k, 0].set_title(\"val img\")\n            ax[k, 1].set_title(\"true label\")\n            ax[k, 2].set_title(\"FCN (cutoff at 0.8)\")\n            ax[k, 3].set_title(\"Compare (Y:tp)\")\n            ax[k, 4].set_title(\"UNET (cutoff at 0.8)\")\n            ax[k, 5].set_title(\"Compare (Y:tp)\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:58.956870Z","iopub.execute_input":"2023-09-09T05:51:58.957301Z","iopub.status.idle":"2023-09-09T05:51:58.974271Z","shell.execute_reply.started":"2023-09-09T05:51:58.957268Z","shell.execute_reply":"2023-09-09T05:51:58.973295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(5)\nval_sample = np.random.choice(n_val, 10)\nval_sample","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:58.976323Z","iopub.execute_input":"2023-09-09T05:51:58.977383Z","iopub.status.idle":"2023-09-09T05:51:58.988696Z","shell.execute_reply.started":"2023-09-09T05:51:58.977341Z","shell.execute_reply":"2023-09-09T05:51:58.987712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prediction by FCN**","metadata":{}},{"cell_type":"code","source":"time1 = time.time()\n\n#prob_val_FCN = predict_probability(X_val[val_sample], model_FCN)\nprob_val_FCN = predict_probability(X_val, model_FCN)\ntime2 = time.time()\n\nval_time_FCN = np.round((time2 - time1), 2)\nprint(val_time_FCN, \"sec\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:51:58.990284Z","iopub.execute_input":"2023-09-09T05:51:58.991103Z","iopub.status.idle":"2023-09-09T05:52:42.014812Z","shell.execute_reply.started":"2023-09-09T05:51:58.991074Z","shell.execute_reply":"2023-09-09T05:52:42.013845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prediction by U-NET**","metadata":{}},{"cell_type":"code","source":"time1 = time.time()\n#prob_val_UNET = predict_probability(X_val[val_sample], model_UNET)\nprob_val_UNET = predict_probability(X_val, model_UNET)\ntime2 = time.time()\n\nval_time_UNET = np.round((time2 - time1), 2)\nprint(val_time_UNET, \"sec\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:52:42.020018Z","iopub.execute_input":"2023-09-09T05:52:42.020314Z","iopub.status.idle":"2023-09-09T05:53:17.874616Z","shell.execute_reply.started":"2023-09-09T05:52:42.020287Z","shell.execute_reply":"2023-09-09T05:53:17.873602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results**\n\nTen of prediction resutls are shown below. Apparently, both models predict somewhat well, although it is not simple task. The true label is **green** and the predicted label is **red**. Predicted label is created by cutting off predicted probability at 0.8. When both are plotted in the same image (refer to **Compare (Y:tp)**), true positive becomes **yellow**. Precisions of prediction are varied, some are very well predicted but some are not. The next question would be \"Does the result depend on dataset?\" ","metadata":{}},{"cell_type":"code","source":"#plot_result(X_val[val_sample], Y_val[val_sample], prob_val_FCN, prob_val_UNET)\nplot_result(X_val[val_sample], Y_val[val_sample], prob_val_FCN[val_sample], prob_val_UNET[val_sample])","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:17.876151Z","iopub.execute_input":"2023-09-09T05:53:17.876511Z","iopub.status.idle":"2023-09-09T05:53:23.215763Z","shell.execute_reply.started":"2023-09-09T05:53:17.876476Z","shell.execute_reply":"2023-09-09T05:53:23.214724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.3 Average IoU\n\nFinally, the average IoU is calculated: predict label, calculate IoU of an image, then take mean of them. Overall, both **FCN and U-NET achieved the target: Average IoU > 0.5 in validation**. When it is calculated by datasets/sources, however, both models did not achieve 0.5 in dataset1 source2. The reason could be lack of quantity of dataset1 source2 (refer to the chapter 4 EDA, section 4.1 Basic Information). To figure out the relationship between number of images and validation scores, scatterplot `Average IoU vs train image quantity` are created. Those are kind of correlated.  ","metadata":{}},{"cell_type":"code","source":"IoU_FCN_list = []\nIoU_UNET_list = []\nfor i in range(n_val):\n    val1 = cal_IoU(Y_val[i], prob_val_FCN[i], cutoff = 0.8)\n    val2 = cal_IoU(Y_val[i], prob_val_UNET[i], cutoff = 0.8)\n    IoU_FCN_list.append(val1)\n    IoU_UNET_list.append(val2)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:23.217123Z","iopub.execute_input":"2023-09-09T05:53:23.217543Z","iopub.status.idle":"2023-09-09T05:53:34.184898Z","shell.execute_reply.started":"2023-09-09T05:53:23.217510Z","shell.execute_reply":"2023-09-09T05:53:34.183899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_tile_df[\"IoU_FCN\"] = IoU_FCN_list\nval_tile_df[\"IoU_UNET\"] = IoU_UNET_list","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:34.186481Z","iopub.execute_input":"2023-09-09T05:53:34.186873Z","iopub.status.idle":"2023-09-09T05:53:34.194741Z","shell.execute_reply.started":"2023-09-09T05:53:34.186837Z","shell.execute_reply":"2023-09-09T05:53:34.192830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Average IoU**","metadata":{}},{"cell_type":"code","source":"val_tile_df[[\"IoU_FCN\", \"IoU_UNET\"]].mean()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:34.196259Z","iopub.execute_input":"2023-09-09T05:53:34.197992Z","iopub.status.idle":"2023-09-09T05:53:34.211390Z","shell.execute_reply.started":"2023-09-09T05:53:34.197954Z","shell.execute_reply":"2023-09-09T05:53:34.210344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Average IoU by dataset/source**","metadata":{}},{"cell_type":"code","source":"mean_IoU_df = val_tile_df[[\"dataset\", \"source_wsi\", \"IoU_FCN\", \"IoU_UNET\"]].groupby([\"dataset\", \"source_wsi\"]).mean()\nmean_IoU_df","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:34.212614Z","iopub.execute_input":"2023-09-09T05:53:34.213030Z","iopub.status.idle":"2023-09-09T05:53:34.229984Z","shell.execute_reply.started":"2023-09-09T05:53:34.212996Z","shell.execute_reply":"2023-09-09T05:53:34.229136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"select = [\"dataset\", \"source_wsi\", \"count\"]\nsum_train_df = tile_df[select].iloc[train_idx].groupby(select[0:-1]).sum().astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:34.231182Z","iopub.execute_input":"2023-09-09T05:53:34.231576Z","iopub.status.idle":"2023-09-09T05:53:34.241124Z","shell.execute_reply.started":"2023-09-09T05:53:34.231542Z","shell.execute_reply":"2023-09-09T05:53:34.240086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum_train_df = sum_train_df.reset_index()\nsum_train_df[\"text\"] = sum_train_df.apply(lambda x: \"d\" +str(x[\"dataset\"]) +\", s\" + str(x[\"source_wsi\"]) , axis = 1) \nsum_train_df = sum_train_df.merge(mean_IoU_df, on = [\"dataset\", \"source_wsi\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:34.242920Z","iopub.execute_input":"2023-09-09T05:53:34.243327Z","iopub.status.idle":"2023-09-09T05:53:34.255515Z","shell.execute_reply.started":"2023-09-09T05:53:34.243291Z","shell.execute_reply":"2023-09-09T05:53:34.254695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (5, 4))\nsns.scatterplot(data =sum_train_df, x = \"count\", y = \"IoU_FCN\", ax =ax, label = \"FCN\")\nsns.scatterplot(data =sum_train_df, x = \"count\", y = \"IoU_UNET\", ax = ax, label = \"U-NET\")\n\nfor i in range(sum_train_df.shape[0]):\n    ax.text(x = sum_train_df[\"count\"].iloc[i] + 12, y = sum_train_df[\"IoU_FCN\"].iloc[i] + 0.04, s= sum_train_df[\"text\"].iloc[i])\nax.set_xlabel(\"number of train images in dataset/source\")\nax.set_ylabel(\"Average IoU\")\nax.grid()\nax.set_xlim(0, 400)\nax.set_ylim(0, 0.7)\nax.set_title(\"Average IoU vs number of train image\")\nax.legend(loc = 4)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:34.256676Z","iopub.execute_input":"2023-09-09T05:53:34.257472Z","iopub.status.idle":"2023-09-09T05:53:34.796143Z","shell.execute_reply.started":"2023-09-09T05:53:34.257437Z","shell.execute_reply":"2023-09-09T05:53:34.795178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.4 IoU Distribution\n\nThose density plots shows IoU Distribution. First one is overall distribution, and latter ones are by dataset/source. For both models, They are normally distributed.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize =(5, 4))\nsns.kdeplot(val_tile_df[\"IoU_FCN\"], ax = ax, label = \"FCN\")\nsns.kdeplot(val_tile_df[\"IoU_UNET\"], ax = ax, label = \"U-NET\")\nax.grid()\nax.legend()\nax.axvline(x = 0.5, ls = \"-.\", color = \"black\", alpha = 0.5)\nax.set_title(\"Validation IoU distribution\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:34.800520Z","iopub.execute_input":"2023-09-09T05:53:34.801364Z","iopub.status.idle":"2023-09-09T05:53:35.312268Z","shell.execute_reply.started":"2023-09-09T05:53:34.801330Z","shell.execute_reply":"2023-09-09T05:53:35.311041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_density_plot(dataset, n_source):\n    \n    fig, ax = plt.subplots(1,n_source, figsize = (3.5*n_source,2.7))\n    \n    for i in range(n_source):\n        filter1 = (val_tile_df[\"dataset\"] == dataset) & (val_tile_df[\"source_wsi\"] == i + 1)\n        sns.kdeplot(val_tile_df[\"IoU_FCN\"].loc[filter1], label = \"FCN IoU\", ax = ax[i])\n        sns.kdeplot(val_tile_df[\"IoU_UNET\"].loc[filter1], label = \"UNET IoU\", ax = ax[i])\n        ax[i].set_xlim(0, 1)\n        ax[i].set_title(\"(dataset, source) = (\" +str(dataset) + \", \" + str(i+1)  + \")\" )\n        ax[i].legend()\n        ax[i].set_xlabel(\"Val IoU\")\n        ax[i].axvline(x = 0.5, ls = \"-.\", color = \"black\", alpha = 0.5)\n        if i > 0:\n            ax[i].set_ylabel(\"\")\n            \n            \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:35.313830Z","iopub.execute_input":"2023-09-09T05:53:35.314211Z","iopub.status.idle":"2023-09-09T05:53:35.329230Z","shell.execute_reply.started":"2023-09-09T05:53:35.314149Z","shell.execute_reply":"2023-09-09T05:53:35.326474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for dataset1/source1, U-NET has peak greater than 0.6.","metadata":{}},{"cell_type":"code","source":"val_density_plot(1, 2)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:35.332243Z","iopub.execute_input":"2023-09-09T05:53:35.332591Z","iopub.status.idle":"2023-09-09T05:53:36.013723Z","shell.execute_reply.started":"2023-09-09T05:53:35.332555Z","shell.execute_reply":"2023-09-09T05:53:36.012751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For dataset2/source1, both models have peak greather than 0.6.","metadata":{}},{"cell_type":"code","source":"val_density_plot(2, 4)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T05:53:36.015315Z","iopub.execute_input":"2023-09-09T05:53:36.020195Z","iopub.status.idle":"2023-09-09T05:53:37.090479Z","shell.execute_reply.started":"2023-09-09T05:53:36.020159Z","shell.execute_reply":"2023-09-09T05:53:37.089571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Kaggle Score\n\n## 8.1 Evaluation Metric\n\nThis competition adopted different metric for scoring. \n\n*Submissions are evaluated by computing the **Average Precision over confidence** scores. ... Segmentation is calculated using **IoU with a threshold of 0.6**.*\n\nhttps://www.kaggle.com/competitions/hubmap-hacking-the-human-vasculature/overview/evaluation\n\nUnlike Average IoU, this metric calculate **true positive and false positive for each blood vessel**. **True positive of a blood vessel means IoU > 0.6**. For example, when there are multiple blood vessels closely exist and predicted connected labels are connected, then it counted as false positive. ","metadata":{}},{"cell_type":"markdown","source":"## 8.2 How I submitted\n\nUsing U-NET or FCN accompanies following additional procedures (**step1 - step3**). Those who applied Mask-RCNN or YOLO does not need step1 - step3. \n\nStep1: To cutoff low probability in predicted mask at threshold 0.8~0.84 (tuned before every submission by IoU). \n\nStep2: To identify all predicted blood vessels indivisually by `cv2.connectedComponents`\n\nStep3: To calculate **mean probability of each component**. It is confidence of prediction required by the competition. \n\nStep4: To encode every predicted label using the provided function [`encode_binary_mask`](https://www.kaggle.com/competitions/hubmap-hacking-the-human-vasculature/overview/evaluation)\n\nStep5: To submit the encoded labels with confidences.","metadata":{}},{"cell_type":"markdown","source":"## 8.3 Kaggle Score\n\nBoth models are trained by 95% of given image, and max_epoch is 35, and the models with lowest validation loss are selected.\nThen kaggle private scores are: \n\n* FCN: 0.264\n* U-NET: 0.255","metadata":{}},{"cell_type":"markdown","source":"# 9. Conclusion\n\nThis project achieved avrage IoU in validation > 0.5 by both FCN and U-NET. First, **EDA discovered imbalance in label** and difference of distribution between datasets/source. Based on EDA, **custom loss function is developed to increase importance of positive labels** which are small in images. **Image augmentation consists horizontal&vertical flip, rotation, and random zoom**, because those operations do not collapse the context (there must be no definitive direction of tissues). It was effective to privent overfitting when there are no plenty of images (only 1600 images are given). \n\nBoth FCN and U-NET took nearly similar computation time in validation: 25~45sec. However, training of FCN was much faster than U-NET. FCN took one epoch for 100sec, whereas U-NET took 210sec, because architectre of FCN is much simpler and number of parameters are be smaller. On the otherhand, U-NET got smaller validation loss at every epoch. Thus, the best architecture depends on circumstances.\n\n**What did not work well**\n\nFollowing attemps did not improve results:\n\n* random brightness change \n* transfer learning from VGG16 trained with imagenet dataset\n\nRandom brightness change is one of typical auguentation, but merely worsened both validation and kaggle score. The reason should be small variation of brightness in dataset.\n\n**Ideas for improvement**\n\nChapter 7.3 revealed that validation score was worst in dataset1/source2, likely due to its small number of images. Since four images were randomly selected for a batch of training input (batch size = 4), dataset1/source2 had relatively less chance to be selected. In this case, **selecting images by equal probability from datasets/source** rectifies this imbalance of training that could improve precision. \n\n","metadata":{}},{"cell_type":"markdown","source":"# References\n\n[1] Fully Convolutional Networks for Semantic Segmentation, Jonathan Long, Evan Shelhamer, Trevor Darrell, UC Berkeley: https://arxiv.org/abs/1411.4038\n\n[2] U-Net: Convolutional Networks for Biomedical Image Segmentation, Olaf Ronneberger, Philipp Fischer, Thomas Brox, Computer Science Department and BIOSS Centre for Biological Signalling Studies,\nUniversity of Freiburg, Germany: https://arxiv.org/abs/1505.04597","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}